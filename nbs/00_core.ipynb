{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlyVec\n",
    "> Functions to generate sparse binary representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "from flyvec.tokenizer import GensimTokenizer\n",
    "from pathlib import Path\n",
    "from functools import cached_property, lru_cache\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def softmax(x: np.array, beta=1.0):\n",
    "    \"\"\"Take the softmax of 1-D vector `x` according to inverse temperature `beta`. Returns a vector of the same length as x\"\"\"\n",
    "    v = np.exp(beta*x)\n",
    "    return v / np.sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10650698, 0.78698604, 0.10650698])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_x = np.array([1,3,1])\n",
    "softmax(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize_synapses(syn: np.array, prec=1.0e-32, p=2):\n",
    "    \"\"\"Normalize the synapses\n",
    "\n",
    "    Args:\n",
    "        syn: The matrix of learned synapses\n",
    "        prec: Noise to prevent division by 0\n",
    "        p: Of the p-norm\n",
    "\n",
    "    Returns:\n",
    "        Normalized array of the given synapses\n",
    "    \"\"\"\n",
    "    K, N = syn.shape\n",
    "    nc = np.power(np.sum(syn**p,axis=1),1/p).reshape(K,1)\n",
    "    return syn / np.tile(nc + prec, (1, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FlyVec:\n",
    "    \"\"\"A class wrapper around a tokenizer, stop words, and synapse weights for hashing words\"\"\"\n",
    "\n",
    "    def __init__(self, synapse_file: Union[Path, str], tokenizer_file: Union[Path, str], stopword_file: Optional[Union[Path, str]]=None, phrases_file: Optional[Union[Path, str]]=None, normalize_synapses: bool=True):\n",
    "\n",
    "        self.synapse_file = str(synapse_file)\n",
    "        self.tokenizer_file = str(tokenizer_file)\n",
    "        self.stopword_file = str(stopword_file) if stopword_file is not None else None\n",
    "        self.phrases_file = str(phrases_file) if phrases_file is not None else None\n",
    "        self.normalize_synapses = normalize_synapses\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, fname: Union[Path, str]):\n",
    "        \"\"\"Create an instance of this class from the configuration present in the `fname` yaml file\"\"\"\n",
    "        fpath = Path(fname)\n",
    "        ref_dir = fpath.parent\n",
    "\n",
    "        with open(fname, \"r\") as fp:\n",
    "            conf = yaml.load(fp, Loader=yaml.FullLoader)\n",
    "\n",
    "        synapse_file = ref_dir / conf[\"synapses\"]\n",
    "        tokenizer_file = ref_dir / conf[\"tokenizer\"]\n",
    "        phrases_file = ref_dir / conf[\"phrases\"] if \"phrases\" in conf.keys() else None\n",
    "        stopword_file = ref_dir / conf[\"stop_words\"] if \"stop_words\" in conf.keys() else None\n",
    "        normalize_synapses = conf.get(\"normalize_synapses\", False)\n",
    "\n",
    "        return cls(synapse_file, tokenizer_file, stopword_file=stopword_file, phrases_file=phrases_file, normalize_synapses=normalize_synapses)\n",
    "\n",
    "    @cached_property\n",
    "    def n_heads(self): return self.synapses.shape\n",
    "\n",
    "    @cached_property\n",
    "    def synapses(self):\n",
    "        \"\"\"The primary weights learned by the model\"\"\"\n",
    "        print(\"Loading synapses...\")\n",
    "        syn = np.load(self.synapse_file)\n",
    "\n",
    "        if self.normalize_synapses: return normalize_synapses(syn)\n",
    "        return syn\n",
    "\n",
    "    @cached_property\n",
    "    def tokenizer(self):\n",
    "        print(\"Loading Tokenizer...\")\n",
    "        return GensimTokenizer.from_file(self.tokenizer_file, self.phrases_file)\n",
    "\n",
    "    @cached_property\n",
    "    def stop_words(self):\n",
    "        \"\"\"Words the model should not respond to\"\"\"\n",
    "        print(\"Loading stop words...\")\n",
    "        return set(np.load(self.stopword_file))\n",
    "\n",
    "    @cached_property\n",
    "    def n_vocab(self): return self.tokenizer.n_vocab()\n",
    "\n",
    "    def make_sentence_vector(self, token_ids: Iterable[int], targ_idx=None, targ_coef=1, targ_coef_is_n_context=False, normalize_vector=True, return_n_context=False, ignore_unknown=True):\n",
    "        \"\"\"Create the input for the synapses given the token ids\n",
    "        \n",
    "        Args:\n",
    "            token_ids: Tokenized input\n",
    "            targ_idx: Which index to treat as the target index. Must be [0, len(token_ids))\n",
    "            targ_coef: Force the target index to have this value\n",
    "            targ_coef_is_n_context: Set target value to number of context. If true, overrides targ_coef.\n",
    "            normalize_vector: If provided, normalize each element by the total number of tokens\n",
    "            return_n_context: Get information about the number of context tokens \n",
    "            ignore_unknown: If the token is not in the vocabulary, do not add it to the sentence vector as \"unknown\"\n",
    "\n",
    "        Returns:\n",
    "            Sentence vector (np.array of shape (N_vocab,))\n",
    "        \"\"\"\n",
    "        H, N = self.synapses.shape\n",
    "        sentence = np.zeros((N,), dtype=np.int8)\n",
    "        vocab = self.tokenizer.dictionary.keys()\n",
    "\n",
    "        def valid_token(t): \n",
    "            is_unknown = ignore_unknown and t != self.tokenizer.patch_dict[\"<UNK>\"]\n",
    "            return t in vocab and t not in self.stop_words and is_unknown\n",
    "\n",
    "        # Assign context\n",
    "        n_context = 0\n",
    "        for i, t in enumerate(token_ids):\n",
    "            if i != targ_idx:\n",
    "                if valid_token(t):\n",
    "                    n_context += 1\n",
    "                    sentence[t] += 1\n",
    "\n",
    "        # Assign target\n",
    "        if targ_coef_is_n_context: \n",
    "            target_value = n_context\n",
    "        else:\n",
    "            target_value = targ_coef\n",
    "\n",
    "        if targ_idx is not None:\n",
    "            target = token_ids[targ_idx]\n",
    "            if target not in self.stop_words:\n",
    "                sentence[self.n_vocab + target] = target_value\n",
    "\n",
    "        if normalize_vector:\n",
    "            divisor = np.sqrt(np.sum(sentence * sentence))\n",
    "            if divisor > 0:\n",
    "                sentence = sentence / divisor # Could be optimized as vector is very sparse\n",
    "\n",
    "        if return_n_context:\n",
    "            out = (sentence, n_context)\n",
    "        else:\n",
    "            out = sentence\n",
    "\n",
    "        return out\n",
    "\n",
    "    def phrase2sentence_vector(self, phrase: str, normalize_vector=True, ignore_unknown=True):\n",
    "        \"\"\"Encode a sentence, then create a context only vector out of a phrase\"\"\"\n",
    "        ids = self.tokenizer.encode(phrase)\n",
    "        return self.make_sentence_vector(ids, normalize_vector=normalize_vector, ignore_unknown=ignore_unknown)\n",
    "\n",
    "    def get_hash_from_token_ids(self, token_ids: List[int], idx: Union[int, None], hash_length: int):\n",
    "        \"\"\"Get hashcode from a set of encoded tokens, selecting the index to use as the target word\n",
    "        \n",
    "        Args:\n",
    "            token_ids: Encoded tokens of length N\n",
    "            idx: Target index to create a hash code for. If None, create a hash of the whole N-Gram.\n",
    "            hash_length: Number of non-zero units in the hash code. Alternatively, number of neurons to accept for hashing\n",
    "\n",
    "        Returns:\n",
    "            {\n",
    "                hash: Desired hash code\n",
    "                activated_neurons: Which neurons fired\n",
    "                activations: \n",
    "                context_attentions: Synapse weights for each fired neuron for the context words\n",
    "                target_attentions: Synapse weights for each fired neuron for the target word\n",
    "            }\n",
    "        \"\"\"\n",
    "        H, N = self.synapses.shape\n",
    "\n",
    "        sentence, n_context = self.make_sentence_vector(token_ids, idx, return_n_context=True)\n",
    "\n",
    "        act = np.dot(self.synapses, sentence)\n",
    "        i_sorted = np.argsort(-act)\n",
    "        act_sort = act[i_sorted]\n",
    "        thr = (act_sort[hash_length - 1] + act_sort[hash_length]) / 2.0\n",
    "        binary = act > thr\n",
    "\n",
    "        # Extract attentions\n",
    "        activated_neurons = i_sorted[:hash_length]\n",
    "        context_ids = token_ids.copy()\n",
    "        context_attentions = self.synapses[activated_neurons][:, context_ids].copy()\n",
    "\n",
    "        if idx is not None:\n",
    "            target_id = token_ids[idx]\n",
    "            target_attentions = self.synapses[activated_neurons][:, self.n_vocab + target_id]\n",
    "            context_attentions[:, idx] = n_context * target_attentions\n",
    "        else:\n",
    "            target_id = None\n",
    "            target_attentions = None\n",
    "\n",
    "        return {\n",
    "            \"hash\": binary.flatten().astype(np.int8),# , attentions\n",
    "            \"activated_neurons\": activated_neurons,\n",
    "            \"activations\": act,\n",
    "            \"context_attentions\": context_attentions,\n",
    "            \"target_attentions\": target_attentions,\n",
    "        }\n",
    "            \n",
    "    def get_hash_from_tokens(self, tokens, targ_token, hash_length):\n",
    "        \"\"\"Get hash from tokens\"\"\"\n",
    "        token_ids = self.tokenizer.tokens2ids(tokens)\n",
    "        targ = self.tokenizer.token2id(targ_token)\n",
    "\n",
    "        try:\n",
    "            idx = token_ids.index(targ)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Specified target token '{targ_token}' not found in '{tokens}'\")\n",
    "\n",
    "        return self.get_hash_from_token_ids(token_ids, idx, hash_length)\n",
    "\n",
    "    def get_hash_from_string(self, string, targ_word, hash_length):\n",
    "        \"\"\"Hash the target word from the string\n",
    "        \n",
    "        Examples:\n",
    "            >>> string1 = 'money in bank checking account'\n",
    "            >>> out = project.get_hash_from_string(string1, \"bank\", 32)\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.encode(string)\n",
    "        targ = self.tokenizer.encode(targ_word)[0]\n",
    "\n",
    "        try:\n",
    "            idx = tokens.index(targ)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Specified target word '{targ_word}' not found in '{self.tokenizer.decode(tokens)}'\")\n",
    "\n",
    "        return self.get_hash_from_token_ids(tokens, idx, hash_length)\n",
    "\n",
    "    def get_k_neighbors(self, query: np.ndarray, k: int, hash_length: int) -> np.ndarray:\n",
    "        \"\"\"Get k nearest context independent codes from query\n",
    "        \n",
    "        Args:\n",
    "            query: Hash code to compare to all other hashes\n",
    "            k: Desired number of nearest neighbors\n",
    "            hash_length: Desired to compare the query against\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        embed = self.get_embeddings(hash_length)\n",
    "        orig_vocabulary = np.dot(query, embed)\n",
    "        ind_sort = np.argsort(-orig_vocabulary)\n",
    "        return self.tokenizer.ids2tokens(ind_sort[:k])\n",
    "\n",
    "    def get_mem_concepts(self, h: int, n_show: int=20, beta: float=800.0):\n",
    "        \"\"\"Retrieve a ranked list of `n` concepts that head `h` learns, weighting them according to inverse temperature `beta`\n",
    "        \n",
    "        Right now, only supports target compartment retrievals\n",
    "        \"\"\"\n",
    "        # context = softmax(self.synapses[h][:self.n_vocab], beta)\n",
    "        target = softmax(self.synapses[h][self.n_vocab:], beta)\n",
    "        return [\n",
    "            {\n",
    "            \"token\": self.tokenizer.id2token(ID),\n",
    "            \"contribution\": float(target[ID])\n",
    "            } for ID in np.argsort(-target)[:n_show]]\n",
    "\n",
    "    @lru_cache(maxsize=8)\n",
    "    def get_embeddings(self, hash_length: int):\n",
    "        \"\"\"Convert all the synapse weights into context independent embeddings. Useful for finding nearest neighbors\n",
    "        \n",
    "        Args:\n",
    "            hash_length: Length of desired hash code\n",
    "\n",
    "        \"\"\"\n",
    "        targets = self.synapses[:, self.n_vocab :]\n",
    "        act_sort = -np.sort(-targets, axis=0)\n",
    "        thr = (act_sort[hash_length - 1, :] + act_sort[hash_length, :]) / 2.0\n",
    "        binary = (targets > thr).astype(np.int8)\n",
    "        return binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are testing multiple configurations of `FlyVec` from different `yaml` files, it can be useful to cache the created objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@lru_cache\n",
    "def initialize_flyvec(fname):\n",
    "    return FlyVec.from_config(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_Tokenizer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flyvec] *",
   "language": "python",
   "name": "conda-env-flyvec-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
