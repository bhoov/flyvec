{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlyVec\n",
    "> Functions that use a pretrained FlyVec model to create sparse binary representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "from flyvec.tokenizer import GensimTokenizer\n",
    "from pathlib import Path\n",
    "from cached_property import cached_property\n",
    "from functools import cached_property, lru_cache\n",
    "from typing import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def softmax(x: np.array, beta=1.0):\n",
    "    \"\"\"Take the softmax of 1-D vector `x` according to inverse temperature `beta`. Returns a vector of the same length as x\"\"\"\n",
    "    v = np.exp(beta*x)\n",
    "    return v / np.sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize_synapses(syn: np.array, prec=1.0e-32, p=2):\n",
    "    \"\"\"Normalize the synapses\n",
    "\n",
    "    Args:\n",
    "        syn: The matrix of learned synapses\n",
    "        prec: Noise to prevent division by 0\n",
    "        p: Of the p-norm\n",
    "\n",
    "    Returns:\n",
    "        Normalized array of the given synapses\n",
    "    \"\"\"\n",
    "    K, N = syn.shape\n",
    "    nc = np.power(np.sum(syn**p,axis=1),1/p).reshape(K,1)\n",
    "    return syn / np.tile(nc + prec, (1, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FlyVec:\n",
    "    \"\"\"A class wrapper around a tokenizer, stop words, and synapse weights for hashing words\"\"\"\n",
    "\n",
    "    def __init__(self, synapse_file: Union[Path, str], tokenizer_file: Union[Path, str], stopword_file: Optional[Union[Path, str]]=None, phrases_file: Optional[Union[Path, str]]=None, normalize_synapses: bool=True):\n",
    "\n",
    "        self.synapse_file = str(synapse_file)\n",
    "        self.tokenizer_file = str(tokenizer_file)\n",
    "        self.stopword_file = str(stopword_file) if stopword_file is not None else None\n",
    "        self.phrases_file = str(phrases_file) if phrases_file is not None else None\n",
    "        self.normalize_synapses = normalize_synapses\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, fname: Union[Path, str]):\n",
    "        \"\"\"Create an instance of this class from the configuration present in the `fname` yaml file\"\"\"\n",
    "        fpath = Path(fname)\n",
    "        ref_dir = fpath.parent\n",
    "\n",
    "        with open(fname, \"r\") as fp:\n",
    "            conf = yaml.load(fp, Loader=yaml.FullLoader)\n",
    "\n",
    "        synapse_file = ref_dir / conf[\"synapses\"]\n",
    "        tokenizer_file = ref_dir / conf[\"tokenizer\"]\n",
    "        phrases_file = ref_dir / conf[\"phrases\"] if \"phrases\" in conf.keys() else None\n",
    "        stopword_file = ref_dir / conf[\"stop_words\"] if \"stop_words\" in conf.keys() else None\n",
    "        normalize_synapses = conf.get(\"normalize_synapses\", False)\n",
    "\n",
    "        return cls(synapse_file, tokenizer_file, stopword_file=stopword_file, phrases_file=phrases_file, normalize_synapses=normalize_synapses)\n",
    "\n",
    "    @cached_property\n",
    "    def n_neurons(self): return self.synapses.shape[0]\n",
    "\n",
    "    @cached_property\n",
    "    def synapses(self):\n",
    "        \"\"\"The primary weights learned by the model\"\"\"\n",
    "        print(\"Loading synapses...\")\n",
    "        syn = np.load(self.synapse_file)\n",
    "\n",
    "        if self.normalize_synapses: return normalize_synapses(syn)\n",
    "        return syn\n",
    "\n",
    "    @cached_property\n",
    "    def tokenizer(self):\n",
    "        print(\"Loading Tokenizer...\")\n",
    "        return GensimTokenizer.from_file(self.tokenizer_file, self.phrases_file)\n",
    "\n",
    "    @cached_property\n",
    "    def stop_words(self):\n",
    "        \"\"\"Words the model should not respond to\"\"\"\n",
    "        print(\"Loading stop words...\")\n",
    "        return set(np.load(self.stopword_file))\n",
    "\n",
    "    @cached_property\n",
    "    def vocab(self):\n",
    "        return set(self.tokenizer.vocab)\n",
    "    \n",
    "    @cached_property\n",
    "    def n_vocab(self): return self.tokenizer.n_vocab()\n",
    "    \n",
    "    @cached_property\n",
    "    def unknown_embedding_info(self):\n",
    "        return {\n",
    "            \"token\": \"<UNK>\",\n",
    "            \"tok_id\": 0,\n",
    "            \"embedding\": np.zeros(self.n_neurons).astype(np.uint8)\n",
    "        }\n",
    "    \n",
    "    def tokenize(self, sentence:str):\n",
    "        return self.tokenizer.tokenize(sentence)\n",
    "    \n",
    "    def is_unknown_token(self, token:str):\n",
    "        \"\"\"Check if a token is unknown (return false) or bad (raise ValueError)\"\"\"\n",
    "        if len(token) == 0:\n",
    "            raise ValueError(\"Token cannot be the empty string\")\n",
    "            \n",
    "        tok = self.tokenizer.tokenize(token)[0]\n",
    "        tok_id = self.tokenizer.token2id(tok)\n",
    "        return tok_id == 0\n",
    "    \n",
    "    def get_sparse_embedding(self, word: str, hash_length: int=32):\n",
    "        \"\"\"Get a context-independent word embedding for a given word. \n",
    "        If, when tokenized, the word is composed of multiple tokens, return the embedding of the first.\n",
    "        \n",
    "        Args:\n",
    "            token: A token (in the vocabulary) to get the word-embedding of\n",
    "            hash_length: The number of non-zero entries in the sparse embedding\n",
    "            normalize_first: If true, preprocess the token to be lowercase, no punctuation, etc. \n",
    "            \n",
    "        Returns:\n",
    "            `np.ndarray` of shape (self.n_neurons,) and dtype np.int8\n",
    "        \"\"\"\n",
    "        if self.is_unknown_token(word): return self.unknown_embedding_info\n",
    "        \n",
    "        dense_info = self.get_dense_embedding(word)\n",
    "        act = dense_info['embedding']\n",
    "        i_sorted = np.argsort(-act)\n",
    "        act_sort = act[i_sorted]\n",
    "        thr = (act_sort[hash_length - 1] + act_sort[hash_length]) / 2.0\n",
    "        binary = (act > thr).astype(np.int8)\n",
    "        return {\n",
    "            \"token\": dense_info['token'],\n",
    "            \"id\": dense_info['id'],\n",
    "            \"embedding\": binary\n",
    "        }\n",
    "    \n",
    "    def get_dense_embedding(self, word: str):\n",
    "        \"\"\"Get a context-independent word embedding for a given token\n",
    "        \n",
    "        Args:\n",
    "            token: A token (in the vocabulary) to get the word-embedding of\n",
    "            hash_length: The number of non-zero entries in the sparse embedding\n",
    "            \n",
    "        Returns:\n",
    "            `np.ndarray` of shape (self.n_neurons,) and dtype np.float64\n",
    "        \"\"\"\n",
    "        if self.is_unknown_token(word): return self.unknown_embedding_info\n",
    "        \n",
    "        token = self.tokenizer.tokenize(word)[0]\n",
    "        tok_id = self.tokenizer.token2id(token)\n",
    "        activation_scores = self.synapses[:, self.n_vocab + tok_id] # Target word embedding is stored in second compartment of matrix\n",
    "\n",
    "        return {\n",
    "            \"token\": token,\n",
    "            \"id\": tok_id,\n",
    "            \"embedding\": activation_scores\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are testing multiple configurations of `FlyVec` from different `yaml` files, it can be useful to cache the created objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer...\n",
      "No phraser specified. Proceeding without phrases\n",
      "Loading synapses...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'token': 'hello',\n",
       " 'id': 5483,\n",
       " 'embedding': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1], dtype=int8)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FlyVec.from_config(\"./model_config.yaml\")\n",
    "hsh = model.get_sparse_embedding(\"hello\", 32); hsh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hsh['embedding']` is non-zero for the top `hash_length` most activated neurons in our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you provide multiple words in the input string, `FlyVec` will provided the word vector for the first word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsh2 = model.get_sparse_embedding(\"hello world\", 32);\n",
    "assert np.all(hsh2['embedding'] == hsh['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_f = lambda x: model.get_sparse_embedding(x)\n",
    "test_eq(_f(\"hello\")['token'], \"hello\")\n",
    "assert np.all(_f(\"BOXNAFS\")['embedding'] == 0), \"Expected unknown embedding to be all zero\"\n",
    "test_eq(_f(\"HELLO\")['embedding'], _f(\"hello\")['embedding'])\n",
    "test_eq(_f(\"not a single token\")['embedding'], _f(\"not\")['embedding'])\n",
    "test_fail(lambda: _f(\"\"), contains=\"empty string\")\n",
    "test_eq(_f(\"NotARealWord\")['embedding'], _f(\"<UNK>\")['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@lru_cache\n",
    "def initialize_flyvec(fname):\n",
    "    return FlyVec.from_config(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flyvec] *",
   "language": "python",
   "name": "conda-env-flyvec-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
