{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train_flyvec\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-barrel",
   "metadata": {},
   "source": [
    "# Training FlyVec\n",
    "\n",
    "FlyVec uses Dense Associative Memory (see [here](https://arxiv.org/abs/1606.01164) and [here](https://arxiv.org/pdf/1908.08993.pdf) to learn associations between target words and their contexts in an entirely unsupervised and hebbian manner. The learned model is a single matrix of shape `(N_SYNAPSES, 2*N_VOCAB)`. \n",
    "\n",
    "The code below requires two vectors: an `encodings` vector (`np.int32`) where each element is a token from your corpus, and an `offsets` vector (`np.uint64`) where each element indicates where each phrase should start. The sliding window for learning associations between target and context words does not cross these barriers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ctypes\n",
    "from ctypes import *\n",
    "import array as arr\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import numpy\n",
    "import flyvec.path_fixes as pf\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Load binaries\n",
    "BIN = pf.CU_BIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# Original argparse implementation\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"encodings_source\", type=str, help=\"\"\"Path to the tokens encoded as an ID. (eltype should be\n",
    "#         np.int32)\"\"\")\n",
    "# parser.add_argument(\"offsets_source\", type=str, help=\"\"\"Path to the offsets array indicating where each value of the\n",
    "# array indicates where a chunk of text should start. (eltype should be np.uint64, first value should be 0 if)\"\"\")\n",
    "# parser.add_argument(\"--output_dir\", \"-o\", default=\".\", type=str, help=\"\"\"Directory in which to save the checkpoints.\n",
    "#         Created if it does not exist\"\"\")\n",
    "# parser.add_argument(\"--save_every\", \"-s\", type=int, default=1, help=\"\"\"How many epochs to run before saving a checkpoint\"\"\")\n",
    "# parser.add_argument(\"--ckpt_prefix\", default=\"flyvec_model_\", type=str, help=\"\"\"Prefix to name each checkpoint.\n",
    "#         Additional parameter choices are inserted into the checkpoint name.\"\"\")\n",
    "# parser.add_argument(\"--starting_checkpoint\", type=str, default=None, help=\"\"\"Path to .npy file of saved checkpoint\"\"\")\n",
    "# parser.add_argument(\"--W\", default=11, type=int, help=\"Size of the W-gram sliding window used to train the word vectors\")\n",
    "# parser.add_argument(\"--hid\", default=400, type=int, help=\"Number of hidden units (neurons). Do not change\")\n",
    "# parser.add_argument(\"--initial_learning_rate\", default=0.0002, type=float, help=\"Initial learning rate\")\n",
    "# parser.add_argument(\"--delta\", default=0, type=float, help=\"From equation\")\n",
    "# parser.add_argument(\"--mu\", default=0, type=float, help=\"\"\"If no checkpoint provided, use this as mean for random normal initialization of synapses\"\"\")\n",
    "# parser.add_argument(\"--sigma\", default=0, type=float, help=\"\"\"If no checkpoint provided, use this as stdev for random normal initialization of synapses\"\"\")\n",
    "# parser.add_argument(\"--Nep\", default=15, type=int, help=\"Maximum number of epochs, fewer if starting from a checkpoint\")\n",
    "# parser.add_argument(\"--batch_size\", \"-b\", default=10000, type=int, help=\"Minibatch size\")\n",
    "# parser.add_argument(\"--prec\", default=1.0E-30, type=float, help=\"Precision, avoid dividing by 0\")\n",
    "# args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-figure",
   "metadata": {},
   "source": [
    "## First compile the code\n",
    "\n",
    "The training code is distributed as c++ files. Let's first ensure it can compile to your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.script import *\n",
    "\n",
    "@call_parse\n",
    "def init():\n",
    "    sp.call([\"sh\", str(BIN / \"short_make\")], cwd=str(BIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "@call_parse\n",
    "def main(\n",
    "    encodings_source:Param(\"\"\"Path to the tokens encoded as an ID. (eltype should be np.int32)\"\"\", str),\n",
    "    offsets_source:Param(\"\"\"Path to the offsets array indicating where each value of the array indicates where a chunk of text should start. (eltype should be np.uint64, first value should be 0)\"\"\",str),\n",
    "    output_dir:Param(\"\"\"Directory in which to save the checkpoints. Created if it does not exist\"\"\", str)=\".\",\n",
    "    save_every:Param(\"\"\"How many epochs to run before saving a checkpoint\"\"\", int)=1,\n",
    "    ckpt_prefix:Param(\"\"\"Prefix to name each checkpoint. Additional parameter choices are inserted into the checkpoint name.\"\"\", str)=\"flyvec_model\",\n",
    "    starting_checkpoint:Param(\"\"\"Path to .npy file of saved checkpoint\"\"\", str)=None,\n",
    "    W:Param(\"Size of the W-gram sliding window used to train the word vectors\", int)=11,\n",
    "    hid:Param(\"Number of hidden units (neurons). Do not change\", int)=400,\n",
    "    initial_learning_rate:Param(\"Initial learning rate\", float)=0.002,\n",
    "    delta:Param(\"From equation\", float)=0.,\n",
    "    mu:Param(\"If no checkpoint provided, use this as mean for random normal initialization of synapses\", float)=0.,\n",
    "    sigma:Param(\"If no checkpoint provided, use this as stdev for random normal initialization of synapses\", float)=0.02,\n",
    "    Nep:Param(\"Maximum number of epochs, fewer if starting from a checkpoint\", int)=15,\n",
    "    batch_size:Param(\"Minibatch size\", int)=10000,\n",
    "    prec:Param(\"Precision, avoid dividing by 0\", float)=1.0E-30,\n",
    "):\n",
    "\n",
    "    # Obsolete Parameters\n",
    "    stride = int(1)\n",
    "    IM_HEIGHT = int(1)\n",
    "    IM_WIDTH = int(11)\n",
    "    Nchannels = int(1)\n",
    "\n",
    "    # Unexposed Parameters\n",
    "    frequency_scaling = 1 # 1 or 0, true or false\n",
    "    Lmid = 1.0\n",
    "    Lbase = 1.0\n",
    "    sparse_input = 1 # 1 or 0, true or false, make input sparse and only store column\n",
    "\n",
    "    # Load object files\n",
    "    model_descriptor = ctypes.CDLL(str(BIN / 'model_descriptor.so'))\n",
    "    model_arrays = ctypes.CDLL(str(BIN / 'model_arrays.so'))\n",
    "    cuda_helpers = ctypes.CDLL(str(BIN / 'cuda_helpers.so'))\n",
    "    model = ctypes.CDLL(str(BIN / 'cuda_funcs.so'))\n",
    "    prune_input = ctypes.CDLL(str(BIN / 'prune_input.so'))\n",
    "    \n",
    "    # m = 2\n",
    "    # p = 2\n",
    "    # parser.add_argument(\"--stride\", default=1, type=int, help=\"Stride. Do not change.\")\n",
    "    # parser.add_argument(\"--IM_HEIGHT\", default=1, type=int, help=\"Height of the image data. Obsolete.\")\n",
    "    # parser.add_argument(\"--IM_WIDTH\", default=11, type=int, help=\"Width of the image data. Obsolete.\")\n",
    "    # parser.add_argument(\"--Nchannels\", default=1, type=int, help=\"Number of channels in the image data. Obsolete.\")\n",
    "    # parser.add_argument(\"--m\", default=2, type=int, help=\"Parameter from equation. Obsolete.\")\n",
    "    # parser.add_argument(\"--p\", default=2, type=int, help=\"Parameter from equation. Obsolete.\")\n",
    "\n",
    "\n",
    "    # Assign values to memory\n",
    "    py_prune_input_data = prune_input.prune_input_data\n",
    "    py_prune_input_data.rgtypes=[c_void_p,c_uint64,c_uint64]\n",
    "\n",
    "    py_getnum_samples = prune_input.getnum_samples\n",
    "    py_getnum_samples.rgtypes=[c_void_p,c_uint64,c_uint64]\n",
    "    py_getnum_samples.restype = c_uint64\n",
    "\n",
    "    py_compute_offset_phrases = prune_input.compute_offset_phrases\n",
    "    py_compute_offset_phrases.rgtypes=[c_void_p, c_void_p, c_uint64, c_uint64]\n",
    "\n",
    "    py_model_create_descriptor = model_descriptor.model_create_descriptor\n",
    "    py_model_create_descriptor.rgtypes=[c_int32]\n",
    "    py_model_create_descriptor.restype = ctypes.c_void_p\n",
    "\n",
    "    py_set_model_param_int = model_descriptor.set_model_param_int\n",
    "    py_set_model_param_float = model_descriptor.set_model_param_float\n",
    "    py_compute_model_derived_parameters = model_descriptor.compute_model_derived_parameters\n",
    "\n",
    "    py_set_model_param_int.rgtypes=[c_uint64,c_char_p,c_void_p]\n",
    "    py_set_model_param_float.rgtypes=[c_float,c_char_p,c_void_p]\n",
    "\n",
    "    py_compute_model_derived_parameters.rgtypes=[c_void_p]\n",
    "\n",
    "    py_copy_model_params = model_descriptor.copy_model_params\n",
    "    py_copy_model_params.rgtypes=[c_void_p,c_int32,c_int32]\n",
    "\n",
    "\n",
    "    py_model_create_arrays = model_arrays.model_create_arrays\n",
    "    py_model_create_arrays.restype = ctypes.c_void_p\n",
    "    py_model_create_arrays.rgtypes=[c_int32]\n",
    "\n",
    "    py_model_arrays_allocate_memory = model_arrays.model_arrays_allocate_memory\n",
    "    py_model_arrays_allocate_memory.rgtypes=[c_void_p,c_void_p,c_int32]\n",
    "\n",
    "\n",
    "    py_model_arrays_reshuffle_indices = model_arrays.do_reshuffle_indices\n",
    "    py_model_arrays_reshuffle_indices.rgtypes=[c_void_p,c_void_p,c_int32]\n",
    "\n",
    "    py_compute_inverse_word_frequency = model_arrays.compute_inverse_word_frequency\n",
    "    py_compute_inverse_word_frequency.rgtypes=[c_void_p,c_void_p,c_void_p,c_uint64,c_int32]\n",
    "\n",
    "    python_get_cuda_pinned_memory = cuda_helpers.do_gpu_cudaHostAlloc\n",
    "    python_get_cuda_pinned_memory.restype = ctypes.c_void_p\n",
    "\n",
    "    python_get_cuda_managed_memory = cuda_helpers.do_gpu_cudaMallocManaged\n",
    "    python_get_cuda_managed_memory.restype = ctypes.c_void_p\n",
    "\n",
    "\n",
    "    py_run_epoch_INPUT_AS_IMAGE = model.launch_epoch_INPUT_AS_IMAGE\n",
    "    py_run_epoch_INPUT_AS_IMAGE.rgtypes=[c_void_p,c_void_p,c_int32,c_uint64,c_uint64]\n",
    "\n",
    "    py_run_epoch_INPUT_AS_FLOAT = model.launch_epoch_INPUT_AS_FLOAT\n",
    "    py_run_epoch_INPUT_AS_FLOAT.rgtypes=[c_void_p,c_void_p,c_int32,c_uint64,c_uint64]\n",
    "\n",
    "\n",
    "    py_run_epoch_INPUT_AS_INT = model.launch_epoch_INPUT_AS_INT\n",
    "    py_run_epoch_INPUT_AS_INT.rgtypes=[c_void_p,c_void_p,c_int32,c_uint64,c_uint64]\n",
    "\n",
    "    py_model_arrays_get_data_pointer = model_arrays.get_data_pointer\n",
    "    py_model_arrays_get_data_pointer.restype = ctypes.c_void_p\n",
    "    py_model_arrays_get_data_pointer.rgtypes = [c_char_p, c_void_p, c_int32]\n",
    "\n",
    "    num_gpus = cuda_helpers.get_cuda_num_devices()\n",
    "    print('num_gpus=', num_gpus)\n",
    "\n",
    "    print(\"py_model_create_arrays and descriptor...\")\n",
    "    MODEL_DSCR = py_model_create_descriptor( c_int32(num_gpus) )\n",
    "    MODEL_DATA = py_model_create_arrays( c_int32(num_gpus) )\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists(): output_dir.mkdir(parents=True)\n",
    "    OUTPUT_NAME=f\"{ckpt_prefix}_H_{hid}_W_{W}_LR_{initial_learning_rate}_\"\n",
    "    OUTPUT = str(output_dir / OUTPUT_NAME)\n",
    "\n",
    "    # Allocate memory for initial encodings\n",
    "    input_data_on_disk_encoding = numpy.load(encodings_source, mmap_mode='r')\n",
    "    Ns_1 = input_data_on_disk_encoding.shape[0]\n",
    "\n",
    "    # Cannot push to GPU\n",
    "    INPUT = python_get_cuda_pinned_memory(ctypes.c_uint64(Ns_1*ctypes.sizeof(ctypes.c_int32)))\n",
    "\n",
    "    #create numpy array to store input data, use memory allocated for INPUT\n",
    "    INPUT_data_pointer = ctypes.cast(INPUT,ctypes.POINTER(ctypes.c_int32))\n",
    "    INPUT_np_array = numpy.ctypeslib.as_array(INPUT_data_pointer,shape=(Ns_1,))\n",
    "\n",
    "    print('Copying input...')\n",
    "    numpy.copyto(INPUT_np_array,input_data_on_disk_encoding)\n",
    "\n",
    "    vocabulary_size = numpy.uint64(numpy.max(INPUT_np_array)+1)\n",
    "    N = numpy.uint64(vocabulary_size*2)\n",
    "    print(f\"Determined a vocabulary size of {vocabulary_size}\")\n",
    "\n",
    "    input_data_on_disk_offsets = numpy.load(offsets_source,mmap_mode='r')\n",
    "    Number_of_sentences = input_data_on_disk_offsets.shape[0] - 1\n",
    "\n",
    "    Number_of_phrases = prune_input.getnum_samples(ctypes.c_void_p(input_data_on_disk_offsets.__array_interface__['data'][0]),\n",
    "            c_uint64(Number_of_sentences), c_uint64(W) )\n",
    "    print('Number of phrases: ', Number_of_phrases)\n",
    "\n",
    "    #allocate memory for offsets for phrases of size W\n",
    "    INPUT_phrases_offsets = python_get_cuda_pinned_memory(ctypes.c_uint64((Number_of_phrases+1)*ctypes.sizeof(ctypes.c_int64) ))\n",
    "\n",
    "    #compute offsets\n",
    "    py_compute_offset_phrases(c_void_p(INPUT_phrases_offsets),\n",
    "            c_void_p(input_data_on_disk_offsets.__array_interface__['data'][0]), c_uint64(Number_of_sentences),\n",
    "            c_uint64(W))\n",
    "\n",
    "    Ns_1 = Number_of_phrases\n",
    "\n",
    "    model_descriptor.print_model_params(ctypes.c_void_p(MODEL_DSCR))\n",
    "\n",
    "    py_set_model_param_int(IM_HEIGHT,b'IM_HEIGHT',ctypes.c_void_p(MODEL_DSCR))\n",
    "\n",
    "    py_set_model_param_int(IM_WIDTH,b'IM_WIDTH',ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_int(Nchannels,b'Nchannels',ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_int(ctypes.c_uint64(Ns_1),b'Ns_1',ctypes.c_void_p(MODEL_DSCR))\n",
    "    #\n",
    "    py_set_model_param_int(W, b'W', ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_int(stride, b'ST', ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_int(hid, b'hid', ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_int(batch_size, b'Num', ctypes.c_void_p(MODEL_DSCR))\n",
    "\n",
    "    py_set_model_param_int(c_uint64(vocabulary_size), b'vocabulary_size',ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_int(sparse_input, b'sparse_input',ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_int(frequency_scaling, b'frequency_scaling',ctypes.c_void_p(MODEL_DSCR))\n",
    "\n",
    "    py_set_model_param_float(c_float(initial_learning_rate), b'initial_learning_rate', ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_float(c_float(delta), b'delta', ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_float(c_float(prec), b'prec', ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_float(c_float(mu), b'mu', ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_float(c_float(sigma), b'sigma', ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_float(c_float(Lmid), b'Lmid', ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_set_model_param_float(c_float(Lbase), b'Lbase', ctypes.c_void_p(MODEL_DSCR))\n",
    "    py_compute_model_derived_parameters(ctypes.c_void_p(MODEL_DSCR))\n",
    "\n",
    "    for gpu in range(1,num_gpus,1):\n",
    "        py_copy_model_params( ctypes.c_void_p(MODEL_DSCR), c_int32(0), c_int32(gpu) )\n",
    "\n",
    "    model_descriptor.print_model_params(ctypes.c_void_p(MODEL_DSCR))\n",
    "\n",
    "    py_model_arrays_allocate_memory(ctypes.c_void_p(MODEL_DSCR), ctypes.c_void_p(MODEL_DATA),c_int32(num_gpus))\n",
    "\n",
    "    print('setting INPUT pointer')\n",
    "    for gpu in range(0,num_gpus,1):\n",
    "        model_arrays.set_up_INPUT_pointer(b'INPUT', ctypes.c_void_p(MODEL_DATA), ctypes.c_void_p(MODEL_DSCR), ctypes.c_void_p(INPUT),ctypes.c_void_p(INPUT_phrases_offsets), b'i4',c_int32(gpu))\n",
    "\n",
    "    if frequency_scaling==1 :\n",
    "        print('Computing inverse word frequency...')\n",
    "        for gpu in range(0,num_gpus,1):\n",
    "            py_compute_inverse_word_frequency(ctypes.c_void_p(MODEL_DATA), ctypes.c_void_p(MODEL_DSCR), ctypes.c_void_p(INPUT), c_uint64(input_data_on_disk_encoding.shape[0]),  c_int32(gpu) )\n",
    "\n",
    "    #model_arrays.push_INPUT_memory_to_GPU(ctypes.c_void_p(MODEL_DATA), ctypes.c_void_p(MODEL_DSCR),c_int32(0), b'i4')\n",
    "\n",
    "    #initialize SYNAPSES\n",
    "    print('Setting initial weights...')\n",
    "    epoch_start = 0\n",
    "    if starting_checkpoint is not None:\n",
    "        R=numpy.load(starting_checkpoint,mmap_mode='r')\n",
    "        # Assume that the epoch_starting_number is the last value of the checkpoint name\n",
    "        epoch_start = int(starting_checkpoint.split(\"_\")[-1])\n",
    "    else:\n",
    "        R = numpy.float32(numpy.random.normal(mu, sigma, (hid,N)))\n",
    "\n",
    "    #push the same initial model data into all GPUs\n",
    "    for gpu in range(0,num_gpus,1):\n",
    "        SYNAPSES_data_pointer = ctypes.cast(py_model_arrays_get_data_pointer(b'synapses',ctypes.c_void_p(MODEL_DATA),c_int32(gpu)),ctypes.POINTER(ctypes.c_float))\n",
    "        SYNAPSES_np_array = numpy.ctypeslib.as_array(SYNAPSES_data_pointer,shape=(hid,N))\n",
    "        numpy.copyto(SYNAPSES_np_array,R)\n",
    "\n",
    "    SYNAPSES_data_pointer = ctypes.cast(py_model_arrays_get_data_pointer(b'synapses',ctypes.c_void_p(MODEL_DATA),c_int32(0)),ctypes.POINTER(ctypes.c_float))\n",
    "    SYNAPSES_np_array = numpy.ctypeslib.as_array(SYNAPSES_data_pointer,shape=(hid,N))\n",
    "\n",
    "    #push INPUT data to the GPU (depending on available memory)\n",
    "    for i, ep in enumerate(range(epoch_start,Nep)):\n",
    "        print('epoch ID = ',ep)\n",
    "        t1 = time.time()\n",
    "        py_model_arrays_reshuffle_indices(ctypes.c_void_p(MODEL_DSCR), ctypes.c_void_p(MODEL_DATA),c_int32(0))\n",
    "        t11 = time.time()\n",
    "        py_run_epoch_INPUT_AS_INT(ctypes.c_void_p(MODEL_DSCR), ctypes.c_void_p(MODEL_DATA), c_int32(num_gpus),\n",
    "                c_uint64(ep), c_uint64(Nep) )\n",
    "        t2 = time.time()\n",
    "        print('time per epoch = ',t2-t1,'[s]', '  py_model_arrays_reshuffle_indices time = ',t11-t1,'[s]')\n",
    "        if ((i+1) % save_every) == 0 or i == (Nep - 1):\n",
    "            numpy.save(OUTPUT+str(ep),SYNAPSES_np_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flyvec] *",
   "language": "python",
   "name": "conda-env-flyvec-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
