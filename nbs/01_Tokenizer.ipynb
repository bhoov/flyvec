{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp tokenizer\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-snowboard",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "> A simple tokenizer for concepts using Gensim\n",
    "\n",
    "Tokenize words using Gensim. We wanted to avoid sub-word tokenization so that we can understand how the model lumps concepts together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import os\n",
    "import regex as re\n",
    "import string\n",
    "from cached_property import cached_property\n",
    "import numpy as np\n",
    "from typing import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-philippines",
   "metadata": {},
   "source": [
    "## Preprocessing functions\n",
    "\n",
    "We want tokens to deal with simple concepts, so we will enforce lowercase ASCII and predominantly split on spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-oriental",
   "metadata": {},
   "source": [
    "Our tokenization will work with \"lines\" -- that is, a sequence of text that can contain multiple sentences, paragraphs, and newlines. For cohesiveness, we want to split these to the sentence and word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"\"\"\n",
    "Various prior work has demonstrated 100 weaknesses in these models — even highly accurate ones — including reliance on non-salient regions \n",
    " or on background information only. Explanation methods help identify these pitfalls by providing explanations for model predictions, enabling humans to identify the features on which a model decision is based. However, these methods provide explanations on the image level making it challenging to understand global model behavior or dataset limitations.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-romania",
   "metadata": {},
   "source": [
    "We first need to check that the line contains actual content and is not a binary string acting as an identifier in most files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_line(line):\n",
    "    \"\"\"Check if the line is valid\"\"\"\n",
    "    return (len(line) > 1) and (\"\\x00\" not in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_good_line(line)\n",
    "assert is_good_line(line)\n",
    "assert not is_good_line(\"\\x00\\x0033-thegreatdivide.txt\\x00\")\n",
    "assert not is_good_line(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-richmond",
   "metadata": {},
   "source": [
    "Split a text by sentence according to the following regex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "spattern = re.compile(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\")\n",
    "\n",
    "def line2sentences(line):\n",
    "    \"\"\"Convert a line into sentences, \"\"\"\n",
    "    line = line.replace('\\n', ' ').strip().lower()\n",
    "    return spattern.split(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-brave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['various prior work has demonstrated 100 weaknesses in these models — even highly accurate ones — including reliance on non-salient regions   or on background information only.',\n",
       " 'explanation methods help identify these pitfalls by providing explanations for model predictions, enabling humans to identify the features on which a model decision is based.',\n",
       " 'however, these methods provide explanations on the image level making it challenging to understand global model behavior or dataset limitations.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = line2sentences(line); sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-carry",
   "metadata": {},
   "source": [
    "Once we have a sentence, we want to strip all punctuation and unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def strip_punc_unicode(line):\n",
    "    \"\"\"Strip all punctuation and unicode from the line\"\"\"\n",
    "    line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    line = ''.join([c for c in line if c.isascii()])\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-sleep",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['various prior work has demonstrated 100 weaknesses in these models  even highly accurate ones  including reliance on nonsalient regions   or on background information only',\n",
       " 'explanation methods help identify these pitfalls by providing explanations for model predictions enabling humans to identify the features on which a model decision is based',\n",
       " 'however these methods provide explanations on the image level making it challenging to understand global model behavior or dataset limitations']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_sentences = [strip_punc_unicode(s) for s in sentences]; proc_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-module",
   "metadata": {},
   "source": [
    "And remove all instances where there are multiple spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "space_pat = re.compile(\"\\s+\")\n",
    "\n",
    "def remove_multiple_spaces(sentence):\n",
    "    return space_pat.sub(\" \", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-figure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['various prior work has demonstrated 100 weaknesses in these models even highly accurate ones including reliance on nonsalient regions or on background information only',\n",
       " 'explanation methods help identify these pitfalls by providing explanations for model predictions enabling humans to identify the features on which a model decision is based',\n",
       " 'however these methods provide explanations on the image level making it challenging to understand global model behavior or dataset limitations']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_sentences = [remove_multiple_spaces(s) for s in proc_sentences]; proc_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-greeting",
   "metadata": {},
   "source": [
    "Before we have our tokens, we will define the concept of 'number' as any ASCII token that contains a digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def isnum(token):\n",
    "    return any(t.isdigit() for t in token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-edinburgh",
   "metadata": {},
   "source": [
    "Compiling all these steps into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def process_line(line):\n",
    "    \"\"\"Compose all transformations to process a line into tokens as desired\"\"\"\n",
    "    sents = line2sentences(line)\n",
    "    out = []\n",
    "    for s in sents:\n",
    "        x = strip_punc_unicode(s)\n",
    "        x = remove_multiple_spaces(x)\n",
    "        xs = x.split()\n",
    "        xs = [x_ if not isnum(x_) else \"<NUM>\" for x_ in xs]\n",
    "        out.append(xs)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-austin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['various', 'prior', 'work', 'has', 'demonstrated', '<NUM>', 'weaknesses', 'in', 'these', 'models', 'even', 'highly', 'accurate', 'ones', 'including', 'reliance', 'on', 'nonsalient', 'regions', 'or', 'on', 'background', 'information', 'only']\n"
     ]
    }
   ],
   "source": [
    "tokens = process_line(line); print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tok(x, num_tok=\"xxNUMxx\", stop_tok=\"xxSTOPxx\", stopwords=[]):\n",
    "    \"\"\"Process a token by replacing numbers and stop tokens with the desired special tokens\"\"\"\n",
    "    if isnum(x):\n",
    "        return num_tok\n",
    "    elif x in stopwords:\n",
    "        return stop_tok\n",
    "    return x.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(process_tok(\" \"), \"\")\n",
    "test_eq(process_tok(\"abc88\"), \"xxNUMxx\")\n",
    "test_eq(process_tok(\"993\"), \"xxNUMxx\")\n",
    "test_eq(process_tok(\"the\", stopwords=[\"the\", \"a\", \"but\"]), \"xxSTOPxx\")\n",
    "test_eq(process_tok(\"   lotsofspace \"), \"lotsofspace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-white",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['various',\n",
       " 'prior',\n",
       " 'work',\n",
       " 'xxSTOPxx',\n",
       " 'demonstrated',\n",
       " '<NUM>',\n",
       " 'weaknesses',\n",
       " 'xxSTOPxx',\n",
       " 'these',\n",
       " 'models',\n",
       " 'even',\n",
       " 'highly',\n",
       " 'accurate',\n",
       " 'ones',\n",
       " 'including',\n",
       " 'reliance',\n",
       " 'xxSTOPxx',\n",
       " 'nonsalient',\n",
       " 'regions',\n",
       " 'xxSTOPxx',\n",
       " 'xxSTOPxx',\n",
       " 'background',\n",
       " 'information',\n",
       " 'only']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[process_tok(t, stopwords=[\"the\", \"in\", \"on\", \"or\", \"has\"]) for t in tokens[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-collins",
   "metadata": {},
   "source": [
    "And now we can convert an entire file to tokens (naively loading everything into memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def file2tokens(fname):\n",
    "    \"\"\"Convert a file of text into tokenized sentences\"\"\"\n",
    "    with open(fname, 'r', encoding='utf8') as fp:\n",
    "        chunk = fp.readlines()\n",
    "        tokenized = []\n",
    "        for line in chunk:\n",
    "            if is_good_line(line):\n",
    "                tokenized += process_line(line)\n",
    "        return tokenized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-mandate",
   "metadata": {},
   "source": [
    "# The Tokenizer\n",
    "> Collecting all the helper functions underneath a single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "PATCH_DICT = {\n",
    "    \"<UNK>\": 0,\n",
    "    \"<NUM>\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GensimTokenizer:\n",
    "    def __init__(self, dictionary, phraser=None, patch_dict=PATCH_DICT):\n",
    "        \"\"\"Wrap a Gensim Dictionary, phrase detector, and special tokens for creating tokenization from OWT\n",
    "        \n",
    "        Args:\n",
    "            dictionary: The gensim dictionary mapping vocabulary to IDs and back\n",
    "            phraser: If provided, use gensim's phrase detector to lump common concepts together\n",
    "            patch_dict: Patch the dictionary with special tokens\n",
    "        \"\"\"\n",
    "        self.dictionary = dictionary\n",
    "        self.phraser = Phrases([[]]) if phraser is None else phraser\n",
    "        self.patch_dict = patch_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, dict_fname, phraser_fname=None):\n",
    "        \"\"\"Load tokenizer information from a dictionary file (generated by gensim dictionary.save) and a phraser file.\"\"\"\n",
    "        d = Dictionary.load(str(dict_fname))\n",
    "        if phraser_fname is not None:\n",
    "            p = Phraser.load(phraser_fname)\n",
    "        else:\n",
    "            print(\"No phraser specified. Proceeding without phrases\")\n",
    "            p = Phraser(Phrases([[]]))\n",
    "            \n",
    "        return cls(d, p)\n",
    "\n",
    "    def add_document_from_fname(self, fname):\n",
    "        \"\"\"For training, add the contents of a text file to the dictionary\"\"\"\n",
    "        print(f\"Adding {fname}\")\n",
    "        tokens = self.phraser[file2tokens(fname)]\n",
    "        self.dictionary.add_documents(tokens)\n",
    "\n",
    "    def add_to_phraser_from_fname(self, fname):\n",
    "        \"\"\"Detect common phrases from fname for bigramming purposes\"\"\"\n",
    "        print(f\"Adding {fname} to phraser\")\n",
    "        tokens = file2tokens(fname)\n",
    "        self.phraser.add_vocab(tokens)\n",
    "\n",
    "    def get_dictionary(self):\n",
    "        return self.dictionary\n",
    "\n",
    "    def token2id(self, word):\n",
    "        \"\"\"Convert a token into an id, converting to UNK ID as necessary\"\"\"\n",
    "        d = self.dictionary\n",
    "        return d.token2id.get(word, d.token2id[\"<UNK>\"])\n",
    "\n",
    "    def tokens2ids(self, tokens):\n",
    "        \"\"\"Convert a list of tokens into ids, converting to UNK as necessary\"\"\"\n",
    "        return [self.token2id(tok) for tok in tokens]\n",
    "\n",
    "    def tokenize(self, s:str):\n",
    "        \"\"\"Convert a sentence into its tokens\"\"\"\n",
    "        return self.phraser[process_line(s)[0]]\n",
    "\n",
    "    def tokenize_batch(self, lines:List[str]):\n",
    "        \"\"\"Convert a batch of lines into their tokens\"\"\"\n",
    "        return self.phraser[[process_line(line)[0] for line in lines]]\n",
    "\n",
    "    def encode(self, s):\n",
    "        \"\"\"Encode a single sentence into IDs\"\"\"\n",
    "        sent_tokens = self.tokenize(s)\n",
    "        return self.tokens2ids(sent_tokens)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Alias for `ids2tokens`\"\"\"\n",
    "        return self.ids2tokens(ids)\n",
    "\n",
    "    def id2token(self, id):\n",
    "        \"\"\"Convert an id to a token\"\"\"\n",
    "        d = self.dictionary\n",
    "        if id == -1: return \"<STOPWRD>\" # Account for post processing\n",
    "        return d[id] # Add error handling if bad id\n",
    "\n",
    "    def ids2tokens(self, ids):\n",
    "        \"\"\"Convert iterable of ids to tokens\"\"\"\n",
    "        return [self.id2token(id) for id in ids]\n",
    "\n",
    "    def set_outdir(self, outdir):\n",
    "        \"\"\"Useful when training in parallel. If set, will save contents to outdir\"\"\"\n",
    "        self.outdir = Path(outdir)\n",
    "\n",
    "    def patch(self, vocab_size, new_vocab, no_below=15, no_above=0.8):\n",
    "        \"\"\"Patch the tokenizer with a manually specified list of tokens, after training\"\"\"\n",
    "        \n",
    "        print(\"Patching with special tokens...\")\n",
    "        self.dictionary.patch_with_special_tokens(self.patch_dict)\n",
    "        print(\"Filtering vocabulary...\")\n",
    "        self.dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=vocab_size)\n",
    "\n",
    "        print(f\"Adding {len(new_vocab)} new words to dictionary...\")\n",
    "        new_vocab = self.tokenize_batch(new_vocab)\n",
    "        self.dictionary.add_documents(new_vocab)\n",
    "        print(f\"Done patching. New vocab size = {self.n_vocab()}\")\n",
    "        return new_vocab\n",
    "\n",
    "    def save(self, outfile):\n",
    "        self.dictionary.save(outfile)\n",
    "\n",
    "    def n_vocab(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    @cached_property\n",
    "    def vocab(self):\n",
    "        return self.dictionary.keys()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_vocab()\n",
    "\n",
    "    def encode_sentences_from_fname(self, fname):\n",
    "        \"\"\"Tokenize all the sentences from a text file\"\"\"\n",
    "        outlist = []\n",
    "        ind_offsets = []\n",
    "        new_start = 0\n",
    "\n",
    "        with open(fname, 'r') as fp:\n",
    "            for line in fp.readlines():\n",
    "                if is_good_line(line):\n",
    "                    sents = self.phraser[process_line(line)]\n",
    "                    for sent in sents:\n",
    "                        ids = self.tokens2ids(sent)\n",
    "                        outlist += ids\n",
    "                        new_start = new_start + len(ids)\n",
    "                        ind_offsets.append(new_start)\n",
    "\n",
    "        return np.asarray(outlist, dtype=np.int32), np.asarray(ind_offsets, dtype=np.uint64)\n",
    "\n",
    "    def encode_and_save_for_mp(self, fname):\n",
    "        \"\"\"Save sentences from fname. Needed because a local function can't be used with the MP module\"\"\"\n",
    "        if self.outdir is None: raise ValueError(\"Please `set_outdir` first\")\n",
    "\n",
    "        fname = Path(fname)\n",
    "\n",
    "        idarr_outfile = self.outdir / (fname.stem + '.npy')\n",
    "        ind_offsets_outfile = self.outdir / (fname.stem + '_offsets.npy')\n",
    "        idarr, ind_offsets = self.encode_sentences_from_fname(fname)\n",
    "        np.save(idarr_outfile, idarr)\n",
    "        np.save(ind_offsets_outfile, ind_offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-retailer",
   "metadata": {},
   "source": [
    "The `GensimTokenizer` is a simple wrapper around gensim's `Dictionary` and `Phraser` classes that aligns them with our simple tokenization rules. Assuming you have a saved Gensim tokenization, you can use the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-filling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No phraser specified. Proceeding without phrases\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2563, 17862, 17, 8073]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = \"../data/tokenizer/gensim1_patched.dict\"\n",
    "tok = GensimTokenizer.from_file(vocab)\n",
    "tokens = [\"apple\", \"pie\", \"is\", \"delicious\"]\n",
    "ids = tok.tokens2ids(tokens); ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-denver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'pie', 'is', 'delicious']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.ids2tokens(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-pittsburgh",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_Tokenizer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flyvec] *",
   "language": "python",
   "name": "conda-env-flyvec-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
